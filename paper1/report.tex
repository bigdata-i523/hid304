\documentclass[sigconf]{acmart}

\input{format/i523}

\begin{document}
\title{Big Data Applications in Astronomy and Astrophysics}


\author{Ricky Carmickle}
 \affiliation{%
   \institution{Indiana University}
   \streetaddress{901 E 10th St}
   \city{Bloomington} 
   \state{Indiana} 
   \postcode{47408}
   \country{USA}}
 \email{rcarmick@umail.iu.edu}


\begin{abstract}

This paper will provide an overview of how Big Data has been applied to existing astrophysics and astronomical datasets. 

It will discuss the two-way relationship between big data and astrophysics which has seen each field advance because of the pressures applied and capabilities developed by the other.  

Improvements in  cameras, which capture the observations from telescopes, have opened the door to more detailed imagery of the sky which can be generated at increasing rates and observation platforms are harnessing this detail into more precise astronomical research projects which challenge every part of the big data pipeline

\end{abstract}

\keywords{i523, hid304, Big Data, Astronomy, Astrophysics, Astroinformatics, Astrostatistics, Large Synoptic Survey Telescope, Square Kilometer Array, Sloan Digital Sky Survey, SkyMapper Southern Sky Survey, Hubble Space Telescope, James Webb Space Telescope, Hough Transform, Renewal String, }

\maketitle

\section{Introduction}
The volume of data generated by astrophysics and astronomical platforms rivals the output of other data sources. Astrophysics and astronomy are considered a primary domain generating 'Big Data', alongside Twitter, YouTube, and Genomics research\cite{Stephens2015}. The growth in volume of astronomical data has been driven by improvements to camera technology, in the sensitivity of sensors, and in the computational resources to gather and store imaging data gathered by astronomy research projects. These are closely related and are now largely collaborative. Universities and researchers from across countries will collaborate with space programs and privatized researchers to create observational platforms which can fulfill the needs of all parties involved. \\

Astronomy's transition into big data began with the release of the Hubble Deep Field (HDF) image in 1995\cite{Andersen2012}. The objects in the HDF were cataloged, and for the first time astronomers accessed a searchable database to return data on objects related to their work rather than the raw images. 

\section{Big Data in Astrophysics}
Astronomic data requires perpetual development of data cleaning, storage, processing, searching, mining, and analysis tools\cite{Borne2014}. 

The foremost challenge is development of search and query tools which can return data relevant to the needs of particular researchers or institutions who participated in the creation of a given sky survey or observational platform. The importance of search functionality is due to the collaborative nature of Astronomical Big Data. The largest observational platforms are expensive in both computing needs and hardware, which incentivizes institutions and agencies to collaborate on projects so that the different needs of astronomers, whatever their particular field of study, can be addressed by searchable databases. Researchers typically focus on particular astronomical phenomena, such as quasars, galaxy formation, exoplanets, etc, and must retrieve data on their specific study area relatively quickly. 

The data collection tool for astrophysics and astronomy data is the telescope. The telescopes applicable to big data, and those which astrophysics research relies on, are either space telescopes placed in orbit or larger terrestrial telescopes. These observe astronomical objects in high definition over a wide range of the electromagnetic spectrum from gamma rays, to visible light, to extremely low-frequency radio waves\cite{DR12017}. 

The largest astronomic and astrophysical research projects have created databases of hundreds of terabytes. Survey platforms in development are expected to capture data in the exabytes\cite{Newman2011,Trader2014,Zhang2015}. \\

The Large Synoptic Survey Telescope (LSST) is the highest volume astronomical data project currently under construction. The LSST is expected to generate 15 terabytes of data per day with over 200 dimensions of data per astronomical object\cite{LSSTRank}. This 3.2 gigapixel telescope camera is located in Cerro Pach√≥n, Chile, and is expected to generate 30 terabytes of data each night of operation for a total of 150 petabytes over the predicted 10-year operational window\cite{EPO}. 

The LSST is designed to fulfill several specific purposes. It will help with understanding dark matter and dark energy, monitoring for potentially hazardous asteroids, studying the outer parts of the solar system, tracking transient objects, and studying the formation of the Milky Way\cite{LSSTRank}. 

The storage and search functions for the terabytes of daily data will be publicly available. The LSST was a collaborative effort between 34 universities and national labs with funding from the National Science Foundation and the Department of Energy Office of Science\cite{EPO}. \\ 


The highest-volume astronomical data project currently in the planning stages is the Square Kilometer Array (SKA)\cite{IBM2012}. 

The SKA is set to be constructed with portions of the array located in South Africa, Australia, and New Zealand. Construction is to begin in 2018 and expected to be completed in 2024. 

The data collection would consist of hundreds of radio telescopes and sensors in an array design spread over thousands of kilometers which will ultimately simulate the sensitivity of a square kilometer telescope. 

This project would gather 14 exabytes of data each day of operation and store about 1 petabyte of that daily data\cite{Economist2010}. 

Transmission of this data would match the entire data output of the internet in 2013\cite{Andersen2012}. 

This project would record radio waves from the 50MHz to 20GHz range in an effort to answer unanswered questions in astrophysics\cite{Project2017}. 

This data will transform the study of the earliest formations in the universe, pulsar physics, properties and physics of galaxies, the role of magnetism in astrophysics, and astrobiology with data split into non-image and image processing. 

Imaging algorithms will need to detect cosmic activity on a scale as short as nanoseconds and record data for events like supernovae and gamma ray bursts. 

Non-image processing will perform pulsar detection and experimentation, which requires processing a massive data stream with computational resources searching for variations in the data, which may indicate a possible pulsar\cite{Project2017}. 

Processing the SKA data will require the use of existing cloud computing resources from nearly all services, and may require rapid advances of desktop cloud computing infrastructure to supplement that of current cloud providers\cite{Newman2011}. 

The SKA is a collaborative effort between ten nations, dozens of universities, and industrial firms. 

It will represent the most data-intensive project of any kind thus far\cite{IBM2012}. \\

Making use of this data has challenged almost every part of the big data field. The processing of imaging data from astronomics and astrophysics platforms is a process of recording high-definition images of the sky, comparing all parts of this image to preceding and successive images to determine the movement of individual objects, then directing the most likely candidates for real astronomical changes to human experts for classification \cite{Enke2012}. The depth and detail of images varies depending on the project goals and wavelength of light being observed. The processing of non-imaging data is a challenge in gathering data streaming from sensors, detecting anomalies in that data, and alerting astrophysicists to potentially important phenomena. Storing and processing astronomic data for searchability has strained the computing resources available to each respective project and prompted developments in computing and storage \cite{IBM2012}. 



\section{Big Data Infrastructure in Astrophysics} 

For ground-based observation projects, the most common source of noise in data which require cleaning are satellites, 'junk' in earth orbit, and defects in the telescope lens which can create artifacts \cite{Storkey2003a}. 

Two of the most effective methods of cleaning astronomical datasets are the Hough Transform method and the Renewal String Approach \cite{Antolovic2008,Storkey2003,Storkey2003a}. Both of these methods are designed to account for satellites, space junk, or aircraft drawing lines across the sky when observed from the ground in successive images. The Hough Transform is a general data mining technique which searches through data, and maps lines based on the placement of points in the sky. Points in subsequent images are matched against the mapped lines. If the number of points mapped to a line are higher than expected, the points are flagged as possible aircraft or satellites.  Renewal String is a method developed specifically for cleaning astronomical datasets. This method searches for line segments formed within a series of images in a given portion of sky imagery. The model compares object movement by renewing images at fixed time intervals and identifying line segments formed by moving objects which identify a noise-generating object\cite{Storkey2003}.\\

Data curation and storage must be handled in an accessible way. Researchers and space agencies across the globe are contributing to projects, and all must be able to access data and potentially upload portions of data to different cloud and open source storage systems\cite{Gannon2014,Stephens2008}. The mining of astronomical data has created an entirely new field of astroinformatics \cite{Borne2009} which focuses on efficient management of computing resources. The resources needed for upcoming astronomy projects, such as the SKA, will require both cutting-edge super computer clusters and cloud computing solutions beyond the current capabilities. Desktop cloud computing networks may provide a complimentary option to help alleviate the computing burden\cite{Newman2011}. The computational volume for astrophysics data is now measured in the hundreds of teraflops. The initial processing of data for terrestrial telescope projects is typically performed on site by supercomputer clusters, which format and compact the raw data. Cloud computing methods are applied to user analysis of cleaned datasets after data is stored for user access\cite{Newman2011}. 

Data mining of astrophysical and astronomical data requires search and selection features which can quickly return data relevant to a researcher's needs. Storage and query structures for modern projects, include established tools like MySQL, SciDB, MonetDB, Hadoop, in addition to project-specific query tools that are perpetually in development\cite{Ivezic2014,Stephens2010}. 

Within these query tools there is development of unique algorithms and complimentary features to optimize query results. The ability to analyze astronomical data is ultimately a problem of identifying significant events, new attributes for astronomical objects, and interesting ``front-page-news\cite{Borne2008}'' outliers using query and analytical tools, which must sift through very large and noisy data. There is ample space for research and development in the search and query task with astronomical datasets. The diversity of astrophysics data provides an environment to challenge query algorithms and the scale of data challenges the most robust query methods. There are many examples of experimental tools for the query and analysis process such as MapReduce\cite{Enke2012} tools for efficiently performing data reduction across computing nodes; XtreemFS\cite{Enke2012}, which provides cloud data replication to improve the accessibility of queried data; and Charles\cite{Sellam2013}, a query advisory system which queries the query system itself to provide a user direction on possible research.
 
 
\section{How Big Data has Changed Astrophysics}
Astronomical and astrophysical data is growing rapidly in size, and researchers are able to query increasingly detailed volumes of data as big data tools develop alongside the observational and recording technology. 

Many of the leading big data tools in astrophysics and astronomy were developed around the Sloan Digital Sky Survey (SDSS)\cite{Economist2010}. SDSS began observations in 1998 and gathered astronomical data as images until 2009. SDSS ultimately generated 140 terabytes of imagery data, which quickly dwarfed the amount of data gathered in the entire history of astronomy\cite{Economist2010}. The fields of Astroinformatics and Astrostatistics\cite{Feigelson2012} emerged as data science caught up to this flow of data. SDSS took imaging data of the visible spectrum and ultimately recorded data for a billion celestial objects including stars, galaxies, and quasars. Although an entire decade of SDSS data will be matched every ten days by the LSST, the SDSS was vital at bringing astronomy and astrophysics into the big data era\cite{Economist2010}. The machine learning, data processing, storing, and querying methods essential to modern astronomy and astrophysics were largely developed concurrent to SDSS. The big data methods developed alongside SDSS were applied to later projects and even retroactively to the data from earlier sky surveys. The big data applications in astronomy are applied primarily to the terrestrial observation platforms for now.

Orbital platforms are limited by the volume of data that can be broadcast to earth and are difficult to classify as big data projects. They are not, however, beyond the reach of developments in big data in astrophysics and astronomy.

The query, cleaning, and analytic methods common to sky surveys are used with data from space telescopes. The Hubble Space Telescope (HST) generates beautiful astronomical imagery and launched astronomy into the big data era with the Hubble Deep Field\cite{HubbleSite.org2016}. The HST, however, returns only 17.5 gigabytes of data a week and that full data is not made publicly available since it is not considered a big data project. The upcoming James Webb Space Telescope (JWST) will implement data compression and reduction as part of its on-board computing process to format data before transmission\cite{Borowitz2011}. It will also not produce data at a volume or level of openness to be considered as much of a big data project.\\




\section{Conclusion}
The LSST and SKA are the most prominent research platforms designed around big data tools which emerged from earlier projects. The SKA is so data intensive that some consider it as much a big data project as an astrophysics project\cite{IBM2012}. There is little indication that astrophysics and astronomy will become less data-intensive in the future. Instead, these fields will continue to push development of infrastructure at all levels of the data science pipeline. Future collaborative efforts may introduce a true big data paradigm to space telescopes to match the embrace of big data in terrestrial observation platforms. 

\begin{acks}
The author would like to thank Dr. Gregor von Laszewski and the I523 TAs for  technical assistance and Dr. Geoffrey Fox for the lecture material which inspired this paper's idea. 
\end{acks}

\bibliographystyle{ACM-Reference-Format}
\bibliography{report}

\end{document}

